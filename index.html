<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Wenye Yu</title>

    <meta name="author" content="Wenye Yu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/sjtu_dorm_icon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Wenye Yu
                </p>
                <p>
                Hi there! I am a first-year Ph.D. student at Machine Vision and Intelligence Group (MVIG), Shanghai Jiao Tong University, advised by Prof.<a href="https://www.mvig.org/">Cewu Lu</a>.
                I received my Bachelor's degree from the first session of Guozhi Class (held by Prof.<a href="https://www.ie.cuhk.edu.hk/faculty/tang-xiaoou-sean/">Xiaoou Tang</a>), Shanghai Jiao Tong University. 
                Previously, I worked as a research intern at <a href="https://github.com/InternRobotics">InternRobotics</a>, supervised by Dr.<a href="https://oceanpang.github.io/">Jiangmiao Pang</a>. 
                </p>
                <p style="text-align:center">
                  <a href="mailto:jason.ywy.0422@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=pN95HCsAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/Jason_ywy">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Virlus/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/yunnan_profile.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/yunnan_profile.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <ul>
                  <li>[2025.04] ðŸŽ‰ One paper (Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation) is accepted by RSS 2025!</li>
                  <li>[2024.09] ðŸŽ‰ One paper (Learning H-Infinity Locomotion Control) is accepted by CoRL 2024!</li>
                  <li>[2023.08] Joined <a href="https://github.com/InternRobotics">InternRobotics</a></li>
                  <li>[2023.02] Joined <a href="https://www.mvig.org/">Machine Vision and Intelligence Group (MVIG)</a></li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interest lies in <strong>Robotics and Embodied AI</strong>, with a particular interest in human-in-the-loop learning and generalizable robotic manipulation.
                  Representative works are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr onmouseout="armada_stop()" onmouseover="armada_start()"  bgcolor="#ffffd0">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='armada_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/armada_tiny_banner.mp4" type="video/mp4">
                  Your browser does not support the video tag.
        
                  </video></div>
                  <img src='images/armada.png' width="160">
                </div>
                <script type="text/javascript">
                  function armada_start() {
                    document.getElementById('armada_image').style.opacity = "1";
                  }
        
                  function armada_stop() {
                    document.getElementById('armada_image').style.opacity = "0";
                  }
                  armada_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2510.02298" id="armada_title">
                  <span class="papertitle">ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation</span>
                </a>
                <br>
                <strong>Wenye Yu</strong>, Jun Lv, Zixi Ying, Yang Jin, Chuan Wenâ€ , Cewu Luâ€ 
                <br>
                <em>In submission</em>, 2025
                <br>
                <a href="https://virlus.github.io/armada/">project page</a> /
                <a href="https://arxiv.org/pdf/2510.02298">paper</a> /
                <a href="https://github.com/Virlus/armada">code</a>
                <p></p>
                <p>ARMADA is a multi-robot deployment and adaptation system with human-in-the-loop shared control, featuring an autonomous online failure detection method named FLOAT.
                  It enables scalable deployment of pretrained policies and thereby expedites policy adaptation to novel scenarios.
                </p>
              </td>
            </tr>

            <tr bgcolor="#ffffd0">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/robosplat.png" alt="PontTuset" width="160" style="border-style: none">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2504.13175" id="robosplat_title">
                  <span class="papertitle">Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation</span>
                </a>
                <br>
                Sizhe Yang*, <strong>Wenye Yu*</strong>, Jia Zeng, Jun Lv, Kerui Ren, Cewu Lu, Dahua Lin, Jiangmiao Pangâ€ 
                <br>
                <em>Robotics: Science and Systems (RSS)</em>, 2025
                <br>
                <a href="https://yangsizhe.github.io/robosplat/">project page</a> /
                <a href="https://arxiv.org/pdf/2504.13175">paper</a> /
                <a href="https://github.com/InternRobotics/RoboSplat">code</a>
                <p></p>
                <p>RoboSplat is framework that leverages 3D Gaussian Splatting (3DGS) to generate novel demonstrations for RGB-based policy learning in a one-shot manner, 
                  enabling robust performance across six types of visual generalization in the real world.</p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/sime.png" alt="PontTuset" width="160" style="border-style: none">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2505.01396" id="sime_title">
                  <span class="papertitle">SIME: Enhancing Policy Self-Improvement with Modal-level Exploration</span>
                </a>
                <br>
                Yang Jin*, Jun Lv*, <strong>Wenye Yu</strong>, Hongjie Fang, Yong-Lu Li, Cewu Luâ€ 
                <br>
                <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2025
                <br>
                <a href="https://ericjin2002.github.io/SIME/">project page</a> /
                <a href="https://arxiv.org/pdf/2505.01396">paper</a> /
                <a href="https://github.com/EricJin2002/SIME">code</a>
                <p></p>
                <p>With modal-level exploration, the robot can generate more diverse and multi-modal interaction data.
                  By learning from the most valuable trials and high-quality segments from these interactions,
                  the robot can effectively refine its capabilities through self-improvement.</p>
              </td>
            </tr>

            <tr bgcolor="#ffffd0">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/hinf.png" alt="PontTuset" width="160" style="border-style: none">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2404.14405" id="hinf_title">
                  <span class="papertitle">Learning H-Infinity Locomotion Control</span>
                </a>
                <br>
                Junfeng Long*, <strong>Wenye Yu*</strong>, Quanyi Li*, Zirui Wang, Dahua Lin, Jiangmiao Pangâ€ 
                <br>
                <em>Conference on Robot Learning (CoRL)</em>, 2024 <br>
                <font color="lightcoral"><strong>Best Poster Award at CoRL 2024 LocoLearn Workshop</strong></font>
                <br>
                <a href="https://junfeng-long.github.io/HINF/">project page</a> /
                <a href="https://arxiv.org/pdf/2404.14405">paper</a>
                <p></p>
                <p>We propose H-Infinity Locomotion Control, an adversarial framework for quadrupedal robots which enhances their robustness against external forces with a performance guarantee.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/grutopia.png" alt="PontTuset" width="160" style="border-style: none">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://www.arxiv.org/abs/2407.10943" id="grutopia_title">
                  <span class="papertitle">GRUtopia: Dream General Robots in a City at Scale</span>
                </a>
                <br>
                Hanqing Wang*, Jiahe Chen*, Wensi Huang*, Qingwei Ben*, Tai Wang*, Boyu Mi*, Tao Huang, Siheng Zhao, Yilun Chen, Sizhe Yang, Peizhou Cao, <strong>Wenye Yu</strong>, Zichao Ye, Jialun Li, Junfeng Long, Zirui Wang, Huiling Wang, Ying Zhao, Zhongying Tu, Yu Qiao, Dahua Lin, Jiangmiao Pangâ€ 
                <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="https://github.com/InternRobotics/InternUtopia">project page</a> /
                <a href="https://www.arxiv.org/pdf/2407.10943">paper</a> /
                <a href="https://github.com/InternRobotics/InternUtopia">code</a>
                <p></p>
                <p>We proposed GRUtopia, the first simulated interactive 3D society designed for various robots. 
                  It features (a)GRScenes, a dataset with 100k interactive and finely annotated scenes. 
                  (b) GRResidents, a LLM driven NPC system. 
                  (c) GRBench, a benchmark posing moderately challenging tasks.</p>
              </td>
            </tr>

          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Awards</h2>
                <ul>
                  <li>[2024.06] SenseTime Scholarship (25 undergraduates per year in China)</li>
                  <li>[2023.09] National Scholarship</li>
                  <li>[2022.09] National Scholarship (Highest Honour for undergraduates in China)</li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Service</h2>
                <p>I served as reviewer for the following conferences: </p>
                <ul>
                  <li>ICRA: 2026</li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          
					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <p>During off time, I am a music enthusiast, fond of a wide range of genres including Hip-Hop, R&B, Rock, Pop, and Electronic. 
              I'm also broadly interested in sports, among which basketball, tennis, and Formula 1 are my favourites.
              Besides, I am also a fan of travelling and cinematography.</p>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This homepage is designed based on <a href="https://jonbarron.info/">Jon Barron's website</a> and deployed on <a href="https://docs.github.com/en/pages">Github Pages</a>.
                  The profile photo is taken by Annie Wang.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
